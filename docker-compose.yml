version: "3.9"

services:
  llama-cpp-server:
    image: local/llama.cpp:full
    ports:
      - 8080:8080
    tty: true
    command: >
      --server
      --model /models/${MODEL_NAME:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}
      --ctx-size ${CONTEXT_SIZE:-2048}
      --threads ${THREADS:-8}
      --mlock --no-mmap --embedding
      --port 8080 --host 0.0.0.0
    volumes:
      - ~/llama.cpp/models:/models
