version: "3.9"

services:
  llama-cpp:
    image: local/llama.cpp:full
    ports:
      - 8080:8080
    tty: true
    command: --server -m /models/phi-2.Q4_0.gguf --mlock --no-mmap --ctx-size 2048 --threads 8 --port 8080 --host 0.0.0.0 --embedding
    volumes:
      - ~/llama.cpp/models:/models
